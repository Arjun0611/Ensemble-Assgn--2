{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10fa54f8-3ebe-4e66-abbf-4950656cd06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0112241-f11f-4ea4-8594-445353dc2fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation of Weak Models: Bagging combines predictions from multiple decision trees.\n",
    "# Random Subsampling: Each decision tree is trained on a random subset of the data with replacement.\n",
    "# Diverse Trees: The randomness in subsampling creates diverse trees.\n",
    "# Reduces Variance: Aggregating diverse trees reduces overfitting by mitigating the impact of individual tree idiosyncrasies.\n",
    "# Improved Generalization: The ensemble's predictions generalize better to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c31182d0-3f4b-4ce9-8dc5-9420c5cd271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.\n",
    "\n",
    "#1. Decision Trees:\n",
    "# Advantages:\n",
    "    # Suitable for capturing complex relationships.\n",
    "    # Robust to outliers.\n",
    "# Disadvantages:\n",
    "    # Prone to overfitting.\n",
    "\n",
    "#2. Linear Models:\n",
    "# Advantages:\n",
    "    # Fast training and prediction.\n",
    "    # Effective for linear relationships.\n",
    "# Disadvantages:\n",
    "    # Limited capacity to capture non-linear patterns.\n",
    "\n",
    "#3. Support Vector Machines (SVM):\n",
    "# Advantages:\n",
    "    # Effective in high-dimensional spaces.\n",
    "    # Robust to outliers.\n",
    "# Disadvantages:\n",
    "    # Computationally intensive.\n",
    "    # Sensitive to parameter tuning.\n",
    "\n",
    "#4. Neural Networks:\n",
    "\n",
    "# Advantages:\n",
    "    # Ability to model intricate patterns.\n",
    "    # Suitable for large datasets.\n",
    "# Disadvantages:\n",
    "    # Prone to overfitting, especially with limited data.\n",
    "    # Computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71429d5c-49db-4d41-84f0-58c3261dfc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.\n",
    "\n",
    "# Flexible Base Learner (e.g., Decision Trees):\n",
    "\n",
    "# Bias: Low\n",
    "# Variance: High\n",
    "# Impact in Bagging: Increases ensemble variance.\n",
    "\n",
    "# Less Flexible Base Learner (e.g., Linear Models):\n",
    "\n",
    "# Bias: Higher\n",
    "# Variance: Lower\n",
    "# Impact in Bagging: Reduces ensemble variance.\n",
    "\n",
    "# Effect in Bagging:\n",
    "\n",
    "# Bagging mitigates high variance of flexible base learners.\n",
    "# Ensemble achieves a better bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff65df3f-37cf-48af-8b32-ad7ac19b3542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.\n",
    "\n",
    "# Bagging in Classification:\n",
    "\n",
    "# Usage:\n",
    "    # Effective for classification tasks.\n",
    "# Aggregation Method:\n",
    "    # Aggregates predictions through majority voting.\n",
    "\n",
    "# Bagging in Regression:\n",
    "\n",
    "# Usage:\n",
    "    # Also applicable for regression tasks.\n",
    "# Aggregation Method:\n",
    "    # Aggregates predictions through averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e029673-0035-4000-b162-e945048fc710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5.\n",
    "\n",
    "# Role of Ensemble Size:\n",
    "\n",
    "# Effect:\n",
    "    # Larger ensemble sizes generally lead to more robust models.\n",
    "# Variance Reduction:\n",
    "    # Increasing ensemble size helps reduce variance.\n",
    "# Stabilization:\n",
    "    # Beyond a certain point, additional models may provide diminishing returns.\n",
    "\n",
    "# Optimal Ensemble Size:\n",
    "\n",
    "# Problem-Dependent:\n",
    "    # Depends on the complexity of the problem.\n",
    "# Empirical Rule:\n",
    "    # Commonly used sizes range from 50 to 500 models.\n",
    "# Tradeoff:\n",
    "    # Balance between computational cost and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e771e6-ced1-43f8-a5be-ccc333364053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6.\n",
    "\n",
    "# Real-World Application: Spam Email Detection\n",
    "# Problem:\n",
    "    # Identify spam emails accurately.\n",
    "# Ensemble Type:\n",
    "    # Bagging with decision tree base learners.\n",
    "# Benefits:\n",
    "    # Reduces overfitting to specific features.\n",
    "    # Enhances generalization to diverse email patterns.\n",
    "    # Improves robustness against evolving spam tactics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
